{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for \"TSM: Temporal Shift Module for Efficient Video Understanding\"\n",
    "# arXiv:1811.08383\n",
    "# Ji Lin*, Chuang Gan, Song Han\n",
    "# {jilin, songhan}@mit.edu, ganchuang@csail.mit.edu\n",
    "\n",
    "# Notice that this file has been modified to support ensemble testing\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import torch.nn.parallel\n",
    "import torch.optim\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from ops.dataset import TSNDataSet\n",
    "from ops.models import TSN\n",
    "from ops.transforms import *\n",
    "from ops import dataset_config\n",
    "from torch.nn import functional as F\n",
    "\n",
    "# options\n",
    "parser = argparse.ArgumentParser(description=\"TSM testing on the full validation set\")\n",
    "parser.add_argument('dataset', type=str)\n",
    "\n",
    "# may contain splits\n",
    "parser.add_argument('--weights', type=str, default=None)\n",
    "parser.add_argument('--test_segments', type=str, default=25)\n",
    "parser.add_argument('--dense_sample', default=False, action=\"store_true\", help='use dense sample as I3D')\n",
    "parser.add_argument('--twice_sample', default=False, action=\"store_true\", help='use twice sample for ensemble')\n",
    "parser.add_argument('--full_res', default=False, action=\"store_true\",\n",
    "                    help='use full resolution 256x256 for test as in Non-local I3D')\n",
    "\n",
    "parser.add_argument('--test_crops', type=int, default=1)\n",
    "parser.add_argument('--coeff', type=str, default=None)\n",
    "parser.add_argument('--batch_size', type=int, default=1)\n",
    "parser.add_argument('-j', '--workers', default=8, type=int, metavar='N',\n",
    "                    help='number of data loading workers (default: 8)')\n",
    "\n",
    "# for true test\n",
    "parser.add_argument('--test_list', type=str, default=None)\n",
    "parser.add_argument('--csv_file', type=str, default=None)\n",
    "\n",
    "parser.add_argument('--softmax', default=False, action=\"store_true\", help='use softmax')\n",
    "\n",
    "parser.add_argument('--max_num', type=int, default=-1)\n",
    "parser.add_argument('--input_size', type=int, default=224)\n",
    "parser.add_argument('--crop_fusion_type', type=str, default='avg')\n",
    "parser.add_argument('--gpus', nargs='+', type=int, default=None)\n",
    "parser.add_argument('--img_feature_dim',type=int, default=256)\n",
    "parser.add_argument('--num_set_segments',type=int, default=1,help='TODO: select multiply set of n-frames from a video')\n",
    "parser.add_argument('--pretrain', type=str, default='imagenet')\n",
    "\n",
    "args = parser.parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "    res = []\n",
    "    for k in topk:\n",
    "         correct_k = correct[:k].view(-1).float().sum(0)\n",
    "         res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "def parse_shift_option_from_log_name(log_name):\n",
    "    if 'shift' in log_name:\n",
    "        strings = log_name.split('_')\n",
    "        for i, s in enumerate(strings):\n",
    "            if 'shift' in s:\n",
    "                break\n",
    "        return True, int(strings[i].replace('shift', '')), strings[i + 1]\n",
    "    else:\n",
    "        return False, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_list = args.weights.split(',')\n",
    "test_segments_list = [int(s) for s in args.test_segments.split(',')]\n",
    "assert len(weights_list) == len(test_segments_list)\n",
    "if args.coeff is None:\n",
    "    coeff_list = [1] * len(weights_list)\n",
    "else:\n",
    "    coeff_list = [float(c) for c in args.coeff.split(',')]\n",
    "\n",
    "if args.test_list is not None:\n",
    "    test_file_list = args.test_list.split(',')\n",
    "else:\n",
    "    test_file_list = [None] * len(weights_list)\n",
    "\n",
    "\n",
    "data_iter_list = []\n",
    "net_list = []\n",
    "modality_list = []\n",
    "\n",
    "total_num = None\n",
    "for this_weights, this_test_segments, test_file in zip(weights_list, test_segments_list, test_file_list):\n",
    "    is_shift, shift_div, shift_place = parse_shift_option_from_log_name(this_weights)\n",
    "    if 'RGB' in this_weights:\n",
    "        modality = 'RGB'\n",
    "    else:\n",
    "        modality = 'Flow'\n",
    "    this_arch = this_weights.split('TSM_')[1].split('_')[2]\n",
    "    modality_list.append(modality)\n",
    "    num_class, args.train_list, val_list, root_path, prefix = dataset_config.return_dataset(args.dataset,\n",
    "                                                                                            modality)\n",
    "    print('=> shift: {}, shift_div: {}, shift_place: {}'.format(is_shift, shift_div, shift_place))\n",
    "    net = TSN(num_class, this_test_segments if is_shift else 1, modality,\n",
    "              base_model=this_arch,\n",
    "              consensus_type=args.crop_fusion_type,\n",
    "              img_feature_dim=args.img_feature_dim,\n",
    "              pretrain=args.pretrain,\n",
    "              is_shift=is_shift, shift_div=shift_div, shift_place=shift_place,\n",
    "              non_local='_nl' in this_weights,\n",
    "              )\n",
    "\n",
    "    if 'tpool' in this_weights:\n",
    "        from ops.temporal_shift import make_temporal_pool\n",
    "        make_temporal_pool(net.base_model, this_test_segments)  # since DataParallel\n",
    "\n",
    "    checkpoint = torch.load(this_weights)\n",
    "    checkpoint = checkpoint['state_dict']\n",
    "\n",
    "    # base_dict = {('base_model.' + k).replace('base_model.fc', 'new_fc'): v for k, v in list(checkpoint.items())}\n",
    "    base_dict = {'.'.join(k.split('.')[1:]): v for k, v in list(checkpoint.items())}\n",
    "    replace_dict = {'base_model.classifier.weight': 'new_fc.weight',\n",
    "                    'base_model.classifier.bias': 'new_fc.bias',\n",
    "                    }\n",
    "    for k, v in replace_dict.items():\n",
    "        if k in base_dict:\n",
    "            base_dict[v] = base_dict.pop(k)\n",
    "\n",
    "    net.load_state_dict(base_dict)\n",
    "\n",
    "    input_size = net.scale_size if args.full_res else net.input_size\n",
    "    if args.test_crops == 1:\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupScale(net.scale_size),\n",
    "            GroupCenterCrop(input_size),\n",
    "        ])\n",
    "    elif args.test_crops == 3:  # do not flip, so only 5 crops\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupFullResSample(input_size, net.scale_size, flip=False)\n",
    "        ])\n",
    "    elif args.test_crops == 5:  # do not flip, so only 5 crops\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupOverSample(input_size, net.scale_size, flip=False)\n",
    "        ])\n",
    "    elif args.test_crops == 10:\n",
    "        cropping = torchvision.transforms.Compose([\n",
    "            GroupOverSample(input_size, net.scale_size)\n",
    "        ])\n",
    "    else:\n",
    "        raise ValueError(\"Only 1, 5, 10 crops are supported while we got {}\".format(args.test_crops))\n",
    "\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "            TSNDataSet(root_path, test_file if test_file is not None else val_list, num_segments=this_test_segments,\n",
    "                       new_length=1 if modality == \"RGB\" else 5,\n",
    "                       modality=modality,\n",
    "                       image_tmpl=prefix,\n",
    "                       test_mode=True,\n",
    "                       remove_missing=len(weights_list) == 1,\n",
    "                       transform=torchvision.transforms.Compose([\n",
    "                           cropping,\n",
    "                           Stack(roll=(this_arch in ['BNInception', 'InceptionV3'])),\n",
    "                           ToTorchFormatTensor(div=(this_arch not in ['BNInception', 'InceptionV3'])),\n",
    "                           GroupNormalize(net.input_mean, net.input_std),\n",
    "                       ]), dense_sample=args.dense_sample, twice_sample=args.twice_sample),\n",
    "            batch_size=args.batch_size, shuffle=False,\n",
    "            num_workers=args.workers, pin_memory=True,\n",
    "    )\n",
    "\n",
    "    if args.gpus is not None:\n",
    "        devices = [args.gpus[i] for i in range(args.workers)]\n",
    "    else:\n",
    "        devices = list(range(args.workers))\n",
    "\n",
    "    net = torch.nn.DataParallel(net.cuda())\n",
    "    net.eval()\n",
    "\n",
    "    data_gen = enumerate(data_loader)\n",
    "\n",
    "    if total_num is None:\n",
    "        total_num = len(data_loader.dataset)\n",
    "    else:\n",
    "        assert total_num == len(data_loader.dataset)\n",
    "\n",
    "    data_iter_list.append(data_gen)\n",
    "    net_list.append(net)\n",
    "\n",
    "\n",
    "output = []\n",
    "\n",
    "\n",
    "def eval_video(video_data, net, this_test_segments, modality):\n",
    "    net.eval()\n",
    "    with torch.no_grad():\n",
    "        i, data, label = video_data\n",
    "        batch_size = label.numel()\n",
    "        num_crop = args.test_crops\n",
    "        if args.dense_sample:\n",
    "            num_crop *= 10  # 10 clips for testing when using dense sample\n",
    "\n",
    "        if args.twice_sample:\n",
    "            num_crop *= 2\n",
    "\n",
    "        if modality == 'RGB':\n",
    "            length = 3\n",
    "        elif modality == 'Flow':\n",
    "            length = 10\n",
    "        elif modality == 'RGBDiff':\n",
    "            length = 18\n",
    "        else:\n",
    "            raise ValueError(\"Unknown modality \"+ modality)\n",
    "\n",
    "        data_in = data.view(-1, length, data.size(2), data.size(3))\n",
    "        if is_shift:\n",
    "            data_in = data_in.view(batch_size * num_crop, this_test_segments, length, data_in.size(2), data_in.size(3))\n",
    "        rst = net(data_in)\n",
    "        rst = rst.reshape(batch_size, num_crop, -1).mean(1)\n",
    "\n",
    "        if args.softmax:\n",
    "            # take the softmax to normalize the output to probability\n",
    "            rst = F.softmax(rst, dim=1)\n",
    "\n",
    "        rst = rst.data.cpu().numpy().copy()\n",
    "\n",
    "        if net.module.is_shift:\n",
    "            rst = rst.reshape(batch_size, num_class)\n",
    "        else:\n",
    "            rst = rst.reshape((batch_size, -1, num_class)).mean(axis=1).reshape((batch_size, num_class))\n",
    "\n",
    "        return i, rst, label\n",
    "\n",
    "\n",
    "proc_start_time = time.time()\n",
    "max_num = args.max_num if args.max_num > 0 else total_num\n",
    "\n",
    "top1 = AverageMeter()\n",
    "top5 = AverageMeter()\n",
    "\n",
    "for i, data_label_pairs in enumerate(zip(*data_iter_list)):\n",
    "    with torch.no_grad():\n",
    "        if i >= max_num:\n",
    "            break\n",
    "        this_rst_list = []\n",
    "        this_label = None\n",
    "        for n_seg, (_, (data, label)), net, modality in zip(test_segments_list, data_label_pairs, net_list, modality_list):\n",
    "            rst = eval_video((i, data, label), net, n_seg, modality)\n",
    "            this_rst_list.append(rst[1])\n",
    "            this_label = label\n",
    "        assert len(this_rst_list) == len(coeff_list)\n",
    "        for i_coeff in range(len(this_rst_list)):\n",
    "            this_rst_list[i_coeff] *= coeff_list[i_coeff]\n",
    "        ensembled_predict = sum(this_rst_list) / len(this_rst_list)\n",
    "\n",
    "        for p, g in zip(ensembled_predict, this_label.cpu().numpy()):\n",
    "            output.append([p[None, ...], g])\n",
    "        cnt_time = time.time() - proc_start_time\n",
    "        prec1, prec5 = accuracy(torch.from_numpy(ensembled_predict), this_label, topk=(1, 5))\n",
    "        top1.update(prec1.item(), this_label.numel())\n",
    "        top5.update(prec5.item(), this_label.numel())\n",
    "        if i % 20 == 0:\n",
    "            print('video {} done, total {}/{}, average {:.3f} sec/video, '\n",
    "                  'moving Prec@1 {:.3f} Prec@5 {:.3f}'.format(i * args.batch_size, i * args.batch_size, total_num,\n",
    "                                                              float(cnt_time) / (i+1) / args.batch_size, top1.avg, top5.avg))\n",
    "\n",
    "video_pred = [np.argmax(x[0]) for x in output]\n",
    "video_pred_top5 = [np.argsort(np.mean(x[0], axis=0).reshape(-1))[::-1][:5] for x in output]\n",
    "\n",
    "video_labels = [x[1] for x in output]\n",
    "\n",
    "\n",
    "if args.csv_file is not None:\n",
    "    print('=> Writing result to csv file: {}'.format(args.csv_file))\n",
    "    with open(test_file_list[0].replace('test_videofolder.txt', 'category.txt')) as f:\n",
    "        categories = f.readlines()\n",
    "    categories = [f.strip() for f in categories]\n",
    "    with open(test_file_list[0]) as f:\n",
    "        vid_names = f.readlines()\n",
    "    vid_names = [n.split(' ')[0] for n in vid_names]\n",
    "    assert len(vid_names) == len(video_pred)\n",
    "    if args.dataset != 'somethingv2':  # only output top1\n",
    "        with open(args.csv_file, 'w') as f:\n",
    "            for n, pred in zip(vid_names, video_pred):\n",
    "                f.write('{};{}\\n'.format(n, categories[pred]))\n",
    "    else:\n",
    "        with open(args.csv_file, 'w') as f:\n",
    "            for n, pred5 in zip(vid_names, video_pred_top5):\n",
    "                fill = [n]\n",
    "                for p in list(pred5):\n",
    "                    fill.append(p)\n",
    "                f.write('{};{};{};{};{};{}\\n'.format(*fill))\n",
    "\n",
    "\n",
    "cf = confusion_matrix(video_labels, video_pred).astype(float)\n",
    "\n",
    "np.save('cm.npy', cf)\n",
    "cls_cnt = cf.sum(axis=1)\n",
    "cls_hit = np.diag(cf)\n",
    "\n",
    "cls_acc = cls_hit / cls_cnt\n",
    "print(cls_acc)\n",
    "upper = np.mean(np.max(cf, axis=1) / cls_cnt)\n",
    "print('upper bound: {}'.format(upper))\n",
    "\n",
    "print('-----Evaluation is finished------')\n",
    "print('Class Accuracy {:.02f}%'.format(np.mean(cls_acc) * 100))\n",
    "print('Overall Prec@1 {:.02f}% Prec@5 {:.02f}%'.format(top1.avg, top5.avg))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
